{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920ef7bb-0e31-4151-9fb0-50de1141d91e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ffe1f7f4ada8c4a3160626f214fc1e36",
     "grade": false,
     "grade_id": "cell-48cccda20e73351e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Hackathon: From Raw Data to ML-Ready Dataset\n",
    "## Insight-Driven EDA and End-to-End Feature Engineering on Airbnb Data Using pandas and Plotly\n",
    "\n",
    "### What is a Hackathon? .\n",
    "\n",
    "A hackathon is a fast-paced, collaborative event where participants use data and technology to solve a real problem end-to-end.  \n",
    "In this hackathon, you will work with a **real-world Airbnb dataset** and complete two interconnected goals.:\n",
    "\n",
    "- Produce a **high-quality exploratory data analysis (EDA)** using `pandas` and `plotly`, extracting meaningful insights, trends, and signals from the data.  \n",
    "- Design and deliver a **clean, feature-rich, ML-ready dataset** that will serve as the foundation for a follow-up hackathon focused on building and evaluating machine learning models.\n",
    "\n",
    "Your task is to **get the most out of the data**: uncover structure and patterns through EDA, and engineer informative features (numerical, categorical, temporal, textual (TF–IDF), and optionally image-based) to maximize the predictive power of the final dataset.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>About the Dataset</b>\n",
    "\n",
    "<u>Context</u>\n",
    "\n",
    "The data comes from <a href=\"https://insideairbnb.com/get-the-data/\">Inside Airbnb</a>, an open project that publishes detailed, regularly updated datasets for cities around the world.  \n",
    "Each city provides three main CSV files:\n",
    "\n",
    "- <b>listings.csv</b> — property characteristics, host profiles, descriptions, amenities, etc.  \n",
    "- <b>calendar.csv</b> — daily availability and pricing information for each listing.  \n",
    "- <b>reviews.csv</b> — guest feedback and textual reviews.\n",
    "\n",
    "These datasets offer a rich view of the short-term rental market, including availability patterns, pricing behavior, host attributes, and guest sentiment.  \n",
    "\n",
    "<u>Inspiration</u>\n",
    "\n",
    "Your ultimate objective is to create a dataset suitable for training a machine learning model that predicts whether a specific Airbnb listing will be <b>available on a given date</b>, using property attributes, review information, and host characteristics.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Task</b>\n",
    "\n",
    "Using one city of your choice from Inside Airbnb, create an end-to-end pipeline that:\n",
    "\n",
    "1. Loads and explores the raw data (EDA).  \n",
    "2. Engineers features (numerical, categorical, temporal, textual TF–IDF, etc.).  \n",
    "3. Builds a unified ML-ready dataset.  \n",
    "\n",
    "Please remember to add comments explaining your decisions. Comments help us understand your thought process and ensure accurate evaluation of your work. This assignment requires code-based solutions—**manually calculated or hard-coded results will not be accepted**. Thoughtful comments and visualizations are encouraged and will be highly valued.\n",
    "\n",
    "- Write your solution directly in this notebook, modifying it as needed.\n",
    "- Once completed, submit the notebook in **.ipynb** format via Moodle.\n",
    "    \n",
    "<b>Collaboration Requirement: Git & GitHub</b>\n",
    "\n",
    "You must collaborate with your team using a **shared GitHub repository**.  \n",
    "Your use of Git is part of the evaluation. We will specifically look at:\n",
    "\n",
    "- Commit quality (clear messages, meaningful steps).  \n",
    "- Balanced participation across team members.  \n",
    "- Use of branches.  \n",
    "- Ability to resolve merge conflicts appropriately.  \n",
    "- A clean, readable project history that reflects real collaboration.\n",
    "\n",
    "Good Git practice is **part of your grade**, not optional.\n",
    "</div>\n",
    "<div class=\"alert alert-danger\">\n",
    "    You are free to add as many cells as you wish as long as you leave untouched the first one.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Hints</b>\n",
    "\n",
    "- Text columns often carry substantial predictive power, use text-vectorization methods to extract meaningful features.  \n",
    "- Make sure all columns use appropriate data types (categorical, numeric, datetime, boolean). Correct dtypes help prevent subtle bugs and improve performance.  \n",
    "- Feel free to enrich the dataset with any additional information you consider useful: engineered features, external data, derived temporal features, etc.  \n",
    "- If the dataset is too large for your computer, use <code>.sample()</code> to work with a subset while preserving the logic of your pipeline.  \n",
    "- Plotly offers a wide variety of powerful visualizations, experiment creatively, but always begin with a clear analytical question: *What insight am I trying to uncover with this plot?*\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<b>Submission Deadline:</b> Wednesday, December 3rd, 12:00\n",
    "\n",
    "Start with a simple, working pipeline.  \n",
    "Do not over-complicate your code too much. Start with a simple working solution and refine it if you have time.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "You may add as many cells as you want, but the **first cell must remain exactly as provided**. Do not edit, move, or delete it under any circumstances.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da41098",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8431230dc8647851888c39d82eb7078d",
     "grade": true,
     "grade_id": "ex1",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84467c34-1fa2-4f03-b7ba-e216836ff6b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7bce797b7aa5f22189e671fd29fa5841",
     "grade": false,
     "grade_id": "cell-140b4c12d85796ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Team Information\n",
    "\n",
    "Fill in the information below.  \n",
    "All fields are **mandatory**.\n",
    "\n",
    "- **GitHub Repository URL**: Paste the link to the team repo you will use for collaboration.\n",
    "- **Team Members**: List all student names (and emails or IDs if required).\n",
    "\n",
    "Do not modify the section title.  \n",
    "Do not remove this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a430c-3e17-42e4-9b63-3bafd596c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Team Information (Mandatory) ===\n",
    "# Fill in the fields below.\n",
    "\n",
    "GITHUB_REPO = \"\"       # e.g. \"https://github.com/myteam/airbnb-hackathon\"\n",
    "TEAM_MEMBERS = [\n",
    "    # \"Full Name 1\",\n",
    "    # \"Full Name 2\",\n",
    "    # \"Full Name 3\",\n",
    "]\n",
    "\n",
    "GITHUB_REPO, TEAM_MEMBERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05ac919",
   "metadata": {},
   "source": [
    "# Airbnb Availability Prediction Pipeline: Barcelona\n",
    "\n",
    "## Overview\n",
    "In this hackathon, we build an **end-to-end machine learning pipeline** to predict whether an Airbnb listing in Barcelona will be available on a given date. We chose Barcelona because it's one of Europe's most vibrant short-term rental markets, with rich data on property characteristics, host behavior, pricing, and guest reviews.\n",
    "\n",
    "Our approach for Monday, 1 December follows three interconnected phases:\n",
    "\n",
    "1. **Exploratory Data Analysis (EDA)** – Understand the data structure, identify patterns, and uncover relationships\n",
    "2. **Feature Engineering** – Create powerful predictive features from raw data\n",
    "3. **ML-Ready Dataset** – Export a clean, normalized dataset ready for machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacebc35",
   "metadata": {},
   "source": [
    "# Phase 1: Exploratory Data Analysis (EDA)\n",
    "\n",
    "## What We're Doing\n",
    "\n",
    "In this phase, we load and explore three raw datasets from Inside Airbnb:\n",
    "\n",
    "- **listings.csv** (~5,000 unique properties) – Property metadata: type, room configuration, amenities, host info, review scores\n",
    "- **calendar.csv** (~7M rows) – Daily availability and pricing for each listing across the entire year\n",
    "- **reviews.csv** (~100k reviews) – Guest feedback, timestamps, and textual comments\n",
    "\n",
    "We merge these datasets at the listing-date level to create a unified table where each row represents a specific listing on a specific date, enriched with all relevant property and host information.\n",
    "\n",
    "### Key EDA Activities:\n",
    "\n",
    "1. **Data Quality Assessment** – Identify missing values, understand data types, detect anomalies\n",
    "2. **Target Variable Analysis** – Examine the distribution of availability (our prediction target)\n",
    "3. **Temporal Patterns** – Discover how availability varies by month, day of week, and season\n",
    "4. **Property Characteristics** – Analyze how property type, size, and amenities relate to availability\n",
    "5. **Host Behavior** – Investigate superhost status, response rates, and listing management\n",
    "6. **Market Signals** – Examine pricing, review scores, and booking patterns\n",
    "7. **Visualizations** – Create interactive plots to communicate insights\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "Good EDA reveals which variables are predictive of availability. For example, we discover that weekend availability is systematically lower (high demand), or luxury properties have different availability strategies than budget listings. These insights directly guide feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71faf1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "# Analyze TARGET VARIABLE: 'available'\n",
    "# This is what we're trying to predict in a machine learning model\n",
    "print(\"\\n=== TARGET VARIABLE: AVAILABILITY ===\")\n",
    "print(f\"\\nUnique values in 'available' column: {df['available'].unique()}\")\n",
    "print(f\"\\nValue counts:\")\n",
    "print(df['available'].value_counts())\n",
    "\n",
    "# Create binary target variable: 1 if available ('t'), 0 if not available ('f')\n",
    "df['is_available'] = (df['available'] == 't').astype(int)\n",
    "\n",
    "print(f\"\\nBinary target distribution:\")\n",
    "print(df['is_available'].value_counts())\n",
    "print(f\"\\nAvailability rate: {df['is_available'].mean()*100:.2f}%\")\n",
    "\n",
    "# Visualization: Availability distribution over time\n",
    "fig = px.histogram(\n",
    "    df,\n",
    "    x='date',\n",
    "    color='is_available',\n",
    "    nbins=100,\n",
    "    title='Availability Over Time (Barcelona Airbnb)',\n",
    "    labels={'is_available': 'Available (1) vs Booked (0)', 'date': 'Date'},\n",
    "    color_discrete_map={0: '#EF553B', 1: '#00CC96'},\n",
    "    barmode='group'\n",
    ")\n",
    "fig.update_layout(height=400, hovermode='x unified')\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n✓ Insight: The availability pattern reveals seasonal demand and booking behavior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a10eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORAL ANALYSIS\n",
    "# Extract temporal features to understand time-based patterns\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day_of_week'] = df['date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "df['day_name'] = df['date'].dt.day_name()\n",
    "df['week'] = df['date'].dt.isocalendar().week\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)  # 1 if Saturday or Sunday\n",
    "\n",
    "print(\"\\n=== TEMPORAL PATTERNS ===\")\n",
    "\n",
    "# Availability by month\n",
    "avail_month = df.groupby('month')['is_available'].agg(['sum', 'count', 'mean'])\n",
    "avail_month.columns = ['Available_Days', 'Total_Days', 'Availability_Rate']\n",
    "print(\"\\nAvailability by Month:\")\n",
    "print(avail_month.round(3))\n",
    "\n",
    "# Visualization: Availability by month\n",
    "fig = px.bar(\n",
    "    x=avail_month.index,\n",
    "    y=avail_month['Availability_Rate'].values,\n",
    "    title='Availability Rate by Month',\n",
    "    labels={'x': 'Month', 'y': 'Availability Rate'},\n",
    "    text=[f\"{v:.2%}\" for v in avail_month['Availability_Rate'].values],\n",
    "    color=avail_month['Availability_Rate'].values,\n",
    "    color_continuous_scale='Blues'\n",
    ")\n",
    "fig.update_traces(textposition='outside')\n",
    "fig.update_layout(height=400)\n",
    "fig.show()\n",
    "\n",
    "# Availability by day of week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "avail_dow = df.groupby('day_name')['is_available'].agg(['sum', 'count', 'mean']).reindex(day_order)\n",
    "print(\"\\nAvailability by Day of Week:\")\n",
    "print(avail_dow.round(3))\n",
    "\n",
    "# Visualization: Availability by day of week\n",
    "fig = px.bar(\n",
    "    x=avail_dow.index,\n",
    "    y=avail_dow['mean'].values,\n",
    "    title='Availability Rate by Day of Week',\n",
    "    labels={'x': 'Day of Week', 'y': 'Availability Rate'},\n",
    "    text=[f\"{v:.2%}\" for v in avail_dow['mean'].values],\n",
    "    color=avail_dow['mean'].values,\n",
    "    color_continuous_scale='Viridis'\n",
    ")\n",
    "fig.update_traces(textposition='outside')\n",
    "fig.update_layout(height=400, xaxis_tickangle=45)\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n✓ Insight: Weekends may show different availability patterns than weekdays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROPERTY CHARACTERISTICS ANALYSIS\n",
    "print(\"\\n=== PROPERTY CHARACTERISTICS ===\")\n",
    "\n",
    "# Property type distribution\n",
    "print(f\"\\nTop 10 Property Types:\")\n",
    "print(df['property_type'].value_counts().head(10))\n",
    "\n",
    "# Room type distribution\n",
    "print(f\"\\nRoom Types:\")\n",
    "print(df['room_type'].value_counts())\n",
    "\n",
    "# Accommodates distribution\n",
    "print(f\"\\nAccommodates (capacity) statistics:\")\n",
    "print(df['accommodates'].describe())\n",
    "\n",
    "# Bedrooms and beds\n",
    "print(f\"\\nBedrooms statistics:\")\n",
    "print(df['bedrooms'].describe())\n",
    "print(f\"\\nBeds statistics:\")\n",
    "print(df['beds'].describe())\n",
    "\n",
    "# Availability by property type (top 8)\n",
    "top_props = df['property_type'].value_counts().head(8).index\n",
    "avail_prop = df[df['property_type'].isin(top_props)].groupby('property_type')['is_available'].mean().sort_values(ascending=False)\n",
    "\n",
    "fig = px.bar(\n",
    "    x=avail_prop.index,\n",
    "    y=avail_prop.values,\n",
    "    title='Availability Rate by Property Type (Top 8)',\n",
    "    labels={'x': 'Property Type', 'y': 'Availability Rate'},\n",
    "    text=[f\"{v:.2%}\" for v in avail_prop.values],\n",
    "    color=avail_prop.values,\n",
    "    color_continuous_scale='RdYlGn'\n",
    ")\n",
    "fig.update_traces(textposition='outside')\n",
    "fig.update_layout(height=400, xaxis_tickangle=45)\n",
    "fig.show()\n",
    "\n",
    "# Availability by room type\n",
    "avail_room = df.groupby('room_type')['is_available'].agg(['mean', 'count'])\n",
    "print(\"\\nAvailability by Room Type:\")\n",
    "print(avail_room.round(3))\n",
    "\n",
    "fig = px.bar(\n",
    "    x=avail_room.index,\n",
    "    y=avail_room['mean'].values,\n",
    "    title='Availability Rate by Room Type',\n",
    "    labels={'x': 'Room Type', 'y': 'Availability Rate'},\n",
    "    text=[f\"{v:.2%}\" for v in avail_room['mean'].values],\n",
    "    color=avail_room['mean'].values,\n",
    "    color_continuous_scale='Blues'\n",
    ")\n",
    "fig.update_traces(textposition='outside')\n",
    "fig.update_layout(height=400)\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n✓ Insight: Different property types have distinct availability patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc27eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOST CHARACTERISTICS ANALYSIS\n",
    "print(\"\\n=== HOST CHARACTERISTICS ===\")\n",
    "\n",
    "# Superhost distribution\n",
    "print(f\"\\nSuperhost Distribution:\")\n",
    "print(df['host_is_superhost'].value_counts())\n",
    "\n",
    "# Response rate\n",
    "print(f\"\\nHost Response Rate Statistics:\")\n",
    "print(df['host_response_rate'].describe())\n",
    "\n",
    "# Listings count\n",
    "print(f\"\\nHost Listings Count Statistics:\")\n",
    "print(df['host_listings_count'].describe())\n",
    "\n",
    "# Availability by superhost status\n",
    "avail_superhost = df.groupby('host_is_superhost')['is_available'].agg(['mean', 'count'])\n",
    "print(\"\\nAvailability by Superhost Status:\")\n",
    "print(avail_superhost.round(3))\n",
    "\n",
    "fig = px.bar(\n",
    "    x=['Non-Superhost', 'Superhost'],\n",
    "    y=avail_superhost['mean'].values,\n",
    "    title='Availability Rate: Superhost vs Non-Superhost',\n",
    "    labels={'y': 'Availability Rate'},\n",
    "    text=[f\"{v:.2%}\" for v in avail_superhost['mean'].values],\n",
    "    color=['#EF553B', '#00CC96']\n",
    ")\n",
    "fig.update_traces(textposition='outside')\n",
    "fig.update_layout(height=400)\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n✓ Insight: Host quality indicators (superhost status) may influence availability strategy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f931c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REVIEW PATTERNS ANALYSIS\n",
    "print(\"\\n=== REVIEW PATTERNS ===\")\n",
    "\n",
    "print(f\"\\nNumber of Reviews per Listing Statistics:\")\n",
    "print(df['n_reviews'].describe())\n",
    "\n",
    "listings_unique = df.drop_duplicates(subset=['listing_id'])\n",
    "print(f\"\\nListings with at least 1 review: {(listings_unique['n_reviews'] > 0).sum()}\")\n",
    "print(f\"Listings with no reviews: {(listings_unique['n_reviews'] == 0).sum()}\")\n",
    "\n",
    "# Review scores rating\n",
    "print(f\"\\nReview Scores Rating Statistics:\")\n",
    "print(df['review_scores_rating'].describe())\n",
    "\n",
    "# Scatter: Reviews vs Rating (using unique listings)\n",
    "fig = px.scatter(\n",
    "    listings_unique[listings_unique['review_scores_rating'].notna()],\n",
    "    x='n_reviews',\n",
    "    y='review_scores_rating',\n",
    "    title='Review Scores vs Number of Reviews',\n",
    "    labels={'n_reviews': 'Number of Reviews', 'review_scores_rating': 'Rating Score'},\n",
    "    opacity=0.5,\n",
    "    color='review_scores_rating',\n",
    "    color_continuous_scale='Blues'\n",
    ")\n",
    "fig.update_layout(height=400)\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n✓ Insight: Review metrics may indicate listing popularity and host reliability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886d59fa",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "This plot highlights whether certain months or specific periods show reduced availability, which typically corresponds to higher demand. Sharp dips in availability may indicate major events, holidays, or weekends with unusually high booking activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef08641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "availability_over_time = (\n",
    "    df.groupby(\"date\", as_index=False)\n",
    "      .agg(availability_rate=(\"available\", lambda x: (x == \"t\").mean()))\n",
    ")\n",
    "\n",
    "fig = px.line(\n",
    "    availability_over_time,\n",
    "    x=\"date\",\n",
    "    y=\"availability_rate\",\n",
    "    title=\"Availability Rate Over Time\",\n",
    "    labels={\"availability_rate\": \"Percent Available\"}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dd5918",
   "metadata": {},
   "source": [
    "Distribution of Listing Prices\n",
    "Understanding the distribution of listing prices is essential for characterizing the overall pricing structure of the market.\n",
    "The distribution reveals whether prices are concentrated around certain values, whether there are multiple pricing segments, and how many properties fall into the premium or budget categories.\n",
    "Since calendar-level prices were not available in this dataset, the listing-level price serves as the primary indicator of cost.\n",
    "This variable typically reflects the base nightly rate set by the host and is therefore an appropriate target for exploratory analysis.\n",
    "The histogram below shows the frequency of listings across different price ranges. The distribution is expected to be right-skewed, with a large number of affordable listings and a long tail representing more expensive properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "[p for p in df.columns if \"price\" in p.lower()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3b5a55",
   "metadata": {},
   "source": [
    "This analysis examines how the availability of listings changes over time. By calculating the proportion of listings that are available on each day, we can uncover seasonal behavior, peak tourism periods, and other fluctuations in demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957fefd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "availability_over_time = (\n",
    "    df.groupby(\"date\", as_index=False)\n",
    "      .agg(availability_rate=(\"available\", lambda x: (x == \"t\").mean()))\n",
    ")\n",
    "\n",
    "fig = px.line(\n",
    "    availability_over_time,\n",
    "    x=\"date\",\n",
    "    y=\"availability_rate\",\n",
    "    title=\"Availability Rate Over Time\",\n",
    "    labels={\"availability_rate\": \"Percent Available\"}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f0a54",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "This plot highlights whether certain months or specific periods show reduced availability, which typically corresponds to higher demand. Sharp dips in availability may indicate major events, holidays, or weekends with unusually high booking activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ea398",
   "metadata": {},
   "outputs": [],
   "source": [
    "[p for p in df.columns if \"price\" in p.lower()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e623435c",
   "metadata": {},
   "source": [
    "Distribution of Listing Prices\n",
    "Understanding the distribution of listing prices is essential for characterizing the overall pricing structure of the market.\n",
    "The distribution reveals whether prices are concentrated around certain values, whether there are multiple pricing segments, and how many properties fall into the premium or budget categories.\n",
    "Since calendar-level prices were not available in this dataset, the listing-level price serves as the primary indicator of cost.\n",
    "This variable typically reflects the base nightly rate set by the host and is therefore an appropriate target for exploratory analysis.\n",
    "The histogram below shows the frequency of listings across different price ranges. The distribution is expected to be right-skewed, with a large number of affordable listings and a long tail representing more expensive properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ff75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['listing_price'] = (\n",
    "    df['price_y']\n",
    "    .astype(str)\n",
    "    .replace('[\\$,]', '', regex=True)\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "df_unique = df.drop_duplicates(\"listing_id\")\n",
    "\n",
    "fig = px.histogram(\n",
    "    df_unique,\n",
    "    x=\"listing_price\",\n",
    "    nbins=50,\n",
    "    title=\"Distribution of Listing Prices\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7b3e5b",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "The distribution provides a clear view of the pricing dynamics within the city.\n",
    "Several patterns typically emerge:\n",
    "The majority of listings cluster within the lower to mid-range price categories, indicating that most hosts position their properties competitively to attract a wide set of guests..\n",
    "The right-skewed tail reflects a smaller number of premium listings that charge substantially higher prices. These may represent luxury apartments, unique accommodations, or properties located in highly desirable areas.\n",
    "The presence of outliers can indicate either genuinely high-value properties or potentially misconfigured price settings by hosts.\n",
    "The width of the distribution suggests how diverse the market is. A wide spread indicates a mix of budget-friendly, mid-range, and high-end listings, whereas a narrow distribution would suggest more uniform pricing.\n",
    "Overall, this distribution helps characterize the market structure and provides a foundation for modeling tasks, such as predicting prices or segmenting listings based on pricing strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c601e7",
   "metadata": {},
   "source": [
    "Availability by Weekday and Month\n",
    "In order to better understand the temporal structure of availability, the availability signal was aggregated by weekday and month.\n",
    "This representation is more compact and interpretable than a day-of-month heatmap, which tends to be visually busy and includes many empty or irregular dates.\n",
    "Aggregating by weekday and month provides answers to questions such as:\n",
    "Are certain weekdays consistently less available than others?\n",
    "Do weekends behave differently from weekdays across the year?\n",
    "Which months show overall lower availability (higher demand)?\n",
    "Do weekday patterns change seasonally?\n",
    "This two-dimensional summary reveals clear behavioral patterns that daily charts often obscure, and it is particularly useful when availability is relatively stable within weeks but varies across seasons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Create month and weekday columns\n",
    "tmp = df.copy()\n",
    "tmp[\"month\"] = tmp[\"date\"].dt.to_period(\"M\").astype(str)\n",
    "tmp[\"weekday\"] = tmp[\"date\"].dt.day_name()\n",
    "\n",
    "# Order weekdays for nicer display\n",
    "weekday_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "tmp[\"weekday\"] = pd.Categorical(tmp[\"weekday\"], categories=weekday_order, ordered=True)\n",
    "\n",
    "# Aggregate availability by month and weekday\n",
    "availability_mw = (\n",
    "    tmp.groupby([\"weekday\", \"month\"], as_index=False)\n",
    "       .agg(availability_rate=(\"available\", lambda x: (x == \"t\").mean()))\n",
    ")\n",
    "\n",
    "# Pivot to weekday rows, month columns\n",
    "pivot_mw = availability_mw.pivot(index=\"weekday\", columns=\"month\", values=\"availability_rate\")\n",
    "\n",
    "fig = px.imshow(\n",
    "    pivot_mw,\n",
    "    aspect=\"auto\",\n",
    "    color_continuous_scale=\"RdBu\",\n",
    "    zmin=0,\n",
    "    zmax=1,\n",
    "    title=\"Availability by Weekday and Month\"\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title=\"Month\")\n",
    "fig.update_yaxes(title=\"Weekday\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b2a195",
   "metadata": {},
   "source": [
    "Interpretation of the Availability by Weekday and Month Heatmap\n",
    "This heatmap summarizes the average availability of listings across weekdays for each month in the dataset.\n",
    "Instead of focusing on daily fluctuations, this view aggregates availability by weekday (Monday to Sunday) and month.\n",
    "Darker red areas indicate lower availability (higher occupancy), while lighter or blue areas indicate higher availability.\n",
    "This visualization is effective for identifying recurring weekly patterns, seasonality across months, and how demand shifts over time.\n",
    "Key Observations\n",
    "1. September 2025 shows the lowest availability overall\n",
    "The darkest red tones appear in September 2025 across all weekdays.\n",
    "This suggests unusually high demand throughout the entire month.\n",
    "Such a pattern may indicate a peak tourism season or a major local event.\n",
    "2. Availability rises sharply from autumn into winter (Nov 2025 – Feb 2026)\n",
    "From November 2025 onward, most cells become significantly lighter.\n",
    "This indicates a shift toward higher availability and lower demand.\n",
    "The market appears much less active during the winter period.\n",
    "3. Early 2026 maintains moderate, stable availability\n",
    "January, February, March, and April 2026 show mostly light blue tones.\n",
    "Availability is more uniform across all weekdays, suggesting consistent demand patterns.\n",
    "No major spikes or dips occur during this period.\n",
    "4. Mid-2026 sees very high availability, indicating reduced demand\n",
    "May 2026 and July 2026 show very pale or near-white tones.\n",
    "This suggests that a large share of listings remained unbooked during this period.\n",
    "Such patterns may occur due to seasonality, oversupply, or reduced tourism activity.\n",
    "5. Weekday differences are relatively small\n",
    "Unlike many Airbnb markets where weekends show clear demand differences, this dataset displays similar availability levels across all weekdays.\n",
    "This indicates demand may be driven more by monthly seasonality rather than day-of-week patterns.\n",
    "Summary of Market Dynamics\n",
    "High demand: September 2025\n",
    "Moderate demand: Late autumn 2025 and early spring 2026\n",
    "Low demand / high availability: Summer 2026 (May–July)\n",
    "Minimal weekday effects: Availability is shaped more by monthly seasonality than by weekly cycles\n",
    "This aggregated weekday–month view provides a clear, high-level understanding of how booking pressure changes across months and helps identify broader seasonal patterns that are difficult to detect in daily-level visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f21a36",
   "metadata": {},
   "source": [
    "Calendar View of Availability for a Single Month\n",
    "While summarizing availability across the entire dataset is useful, it can also be valuable to focus on a single month and visualize the availability pattern in a calendar-like layout.\n",
    "In this representation:\n",
    "Columns correspond to days of the week.\n",
    "Rows correspond to weeks within the month.\n",
    "Each cell represents the average availability for that weekday/week combination.\n",
    "This format closely resembles a standard monthly calendar and is effective for presenting a detailed view of booking pressure during a specific period, such as a peak tourism month or a month containing major events.\n",
    "This visualization is more illustrative than analytical, but it helps communicate concrete patterns in a simple, intuitive format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e8a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# choose a month you care about\n",
    "target_month = \"2025-09\"  # change if needed\n",
    "\n",
    "month_df = df[df[\"date\"].dt.to_period(\"M\") == target_month].copy()\n",
    "\n",
    "month_df[\"weekday\"] = month_df[\"date\"].dt.dayofweek  # Monday=0\n",
    "month_df[\"week_of_month\"] = ((month_df[\"date\"].dt.day - 1) // 7)  # 0,1,2,3,4\n",
    "\n",
    "# aggregate availability per cell\n",
    "cal = (\n",
    "    month_df.groupby([\"week_of_month\", \"weekday\"], as_index=False)\n",
    "            .agg(availability_rate=(\"available\", lambda x: (x == \"t\").mean()))\n",
    ")\n",
    "\n",
    "# build matrix 5 weeks × 7 days\n",
    "calendar_matrix = np.full((cal[\"week_of_month\"].max()+1, 7), np.nan)\n",
    "for _, row in cal.iterrows():\n",
    "    calendar_matrix[int(row[\"week_of_month\"]), int(row[\"weekday\"])] = row[\"availability_rate\"]\n",
    "\n",
    "fig = px.imshow(\n",
    "    calendar_matrix,\n",
    "    color_continuous_scale=\"RdBu\",\n",
    "    zmin=0,\n",
    "    zmax=1,\n",
    "    title=f\"Availability Calendar Heatmap – {target_month}\",\n",
    ")\n",
    "\n",
    "fig.update_xaxes(\n",
    "    tickmode=\"array\",\n",
    "    tickvals=list(range(7)),\n",
    "    ticktext=[\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"],\n",
    "    title=\"Day of Week\",\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    tickmode=\"array\",\n",
    "    tickvals=list(range(calendar_matrix.shape[0])),\n",
    "    ticktext=[f\"Week {i+1}\" for i in range(calendar_matrix.shape[0])],\n",
    "    title=\"Week of Month\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec824820",
   "metadata": {},
   "source": [
    "Interpretation of the Availability Calendar Heatmap (September 2025)\n",
    "The calendar-style heatmap provides a detailed view of listing availability for the selected month (September 2025), broken down by day of the week and week of the month. Each cell represents the average share of listings that were available on that weekday during the corresponding week. Darker red cells indicate lower availability (higher occupancy), while lighter or blue tones indicate higher availability.\n",
    "Several patterns emerge from the visualization:\n",
    "1. Weeks 1 and 2 show unusually high availability\n",
    "Most cells in the first half of the month are very light in color.\n",
    "This indicates that many listings remained available, reflecting lower demand.\n",
    "This pattern aligns with the end of the summer holiday season when travel volumes typically decline.\n",
    "2. Week 3 displays the lowest availability across the month\n",
    "The third week contains several dark red cells, especially on Monday and Sunday.\n",
    "This suggests a period of unusually high demand.\n",
    "The pattern may be associated with an event, festival, or other travel-intensive period.\n",
    "3. Week 4 remains moderately booked\n",
    "Availability increases slightly compared to Week 3 but remains lower than in early September.\n",
    "This indicates a gradual easing of demand after the peak in Week 3.\n",
    "4. Week 5 shows mixed patterns\n",
    "The beginning of Week 5 shows moderate availability.\n",
    "Later days in the week appear empty or neutral, likely due to missing data for dates beyond the dataset range.\n",
    "The earlier part of the week suggests availability recovering toward the end of the month.\n",
    "5. Weekend effects are visible\n",
    "Sunday in Week 2 is one of the darkest cells, indicating strong booking pressure.\n",
    "This may reflect common turnover cycles (check-ins or check-outs) or an event concentrated on that specific date.\n",
    "Summary of September 2025 Booking Dynamics\n",
    "Low demand in Weeks 1 and 2.\n",
    "A sharp and pronounced peak in demand during Week 3.\n",
    "Moderate demand in Week 4.\n",
    "Mixed availability in Week 5, partially influenced by incomplete data.\n",
    "This visualization effectively highlights how booking behavior varies both across and within weeks. It also illustrates how certain periods experience concentrated demand, which can be valuable for understanding seasonal patterns, event-driven spikes, and overall market dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81cb87c",
   "metadata": {},
   "source": [
    "# Phase 2: Feature Engineering & Phase 3: ML-Ready Dataset\n",
    "\n",
    "## Phase 2: What We're Engineering\n",
    "\n",
    "Feature engineering transforms raw data into **predictive features** that improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3909c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_calendar = pd.read_csv(r\"C:\\Users\\amatm\\OneDrive\\Escriptori\\p\\calendar.csv.gz\", compression=\"gzip\")\n",
    "df_listings = pd.read_csv(r\"C:\\Users\\amatm\\OneDrive\\Escriptori\\p\\listings.csv.gz\", compression=\"gzip\")\n",
    "df_reviews = pd.read_csv(r\"C:\\Users\\amatm\\OneDrive\\Escriptori\\p\\reviews.csv.gz\", compression=\"gzip\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042075f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ff56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56801c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042abc57",
   "metadata": {},
   "source": [
    "### Merging calendar, listings and aggregated reviews\n",
    "\n",
    "We merge the `calendar`, `listings`, and aggregated `reviews` datasets to create a single table that contains daily availability, property characteristics, and guest feedback.  \n",
    "This unified dataset is essential because the model needs all three types of information to learn which listings are more or less likely to be available on a given date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74331c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = df_calendar.merge(\n",
    "    df_listings,\n",
    "    how=\"left\",\n",
    "    left_on=\"listing_id\",\n",
    "    right_on=\"id\"\n",
    ")\n",
    "\n",
    "df = df.drop(columns=[\"id\"])\n",
    "\n",
    "df_reviews_agg = df_reviews.groupby(\"listing_id\").agg(\n",
    "    n_reviews=(\"id\", \"count\"),\n",
    "    first_review=(\"date\", \"min\"),\n",
    "    last_review=(\"date\", \"max\")\n",
    ").reset_index()\n",
    "\n",
    "df = df.merge(\n",
    "    df_reviews_agg,\n",
    "    how=\"left\",\n",
    "    on=\"listing_id\"\n",
    ")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ed5ff",
   "metadata": {},
   "source": [
    "### Initial dataset inspection\n",
    "\n",
    "We create a working copy of the dataset and inspect its structure, data types, and missing values.  \n",
    "This step is important because it gives us an initial understanding of the data quality and helps identify potential issues that could affect the model's ability to learn availability patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd3614",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work = df.copy()\n",
    "\n",
    "display(df_work.head())\n",
    "\n",
    "df_work.info()\n",
    "\n",
    "df_work[\"available\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f22a12",
   "metadata": {},
   "source": [
    "### Cleaning duplicated merge columns\n",
    "\n",
    "After merging the datasets, several variables appeared with `_x` and `_y` suffixes.  \n",
    "We standardize these column names and remove redundant duplicates to avoid confusion and ensure that each feature is represented only once.  \n",
    "This is imp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc938c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_cols = {\n",
    "    \"price_x\": \"price\",\n",
    "    \"minimum_nights_x\": \"minimum_nights\",\n",
    "    \"maximum_nights_x\": \"maximum_nights\",\n",
    "    \"first_review_y\": \"first_review\",\n",
    "    \"last_review_y\": \"last_review\",\n",
    "}\n",
    "\n",
    "df_work = df_work.rename(columns=rename_cols)\n",
    "\n",
    "cols_to_drop = [c for c in df_work.columns if c.endswith(\"_y\") and c not in rename_cols.values()]\n",
    "\n",
    "df_work = df_work.drop(columns=cols_to_drop)\n",
    "\n",
    "df_work.filter(list(rename_cols.values())).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312a2a15",
   "metadata": {},
   "source": [
    "### Removing non-informative identification and metadata fields\n",
    "\n",
    "We drop ID fields, URLs, and scraping metadata because they do not contain any meaningful information related to a listing’s availability.  \n",
    "Keeping these variables would introduce noise and increase the risk of overfitting, since IDs and URLs are arbitrary and scraping metadata has no causal relationship with whether a listing is available on a given date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96292d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_cols = [\n",
    "    \"id\", \n",
    "    \"listing_url\",\n",
    "    \"scrape_id\",\n",
    "    \"picture_url\",\n",
    "    \"thumbnail_url\",\n",
    "    \"medium_url\",\n",
    "    \"xl_picture_url\",\n",
    "    \"host_url\",\n",
    "    \"host_thumbnail_url\",\n",
    "    \"host_picture_url\",\n",
    "    \n",
    "    # Metadatos de scraping / calendario\n",
    "    \"last_scraped\",\n",
    "    \"calendar_last_scraped\",\n",
    "    \"calendar_updated\",\n",
    "    \n",
    "    # Otros muy específicos poco útiles para availability\n",
    "    \"license\",\n",
    "]\n",
    "\n",
    "# Nos quedamos solo con las que realmente existan en el df\n",
    "irrelevant_cols = [c for c in irrelevant_cols if c in df_work.columns]\n",
    "\n",
    "df_work = df_work.drop(columns=irrelevant_cols)\n",
    "\n",
    "df_work.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f2c6fe",
   "metadata": {},
   "source": [
    "### Identifying missing data patterns\n",
    "\n",
    "We compute the percentage of missing values for every column to understand the overall data quality and detect variables that may not be useful for the model.  \n",
    "This step is important because features with extremely high levels of missing data can distort the learning process and should be removed or handled carefully before defining the final dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c1bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_percent = df_work.isna().mean().sort_values(ascending=False) * 100\n",
    "na_percent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429357e1",
   "metadata": {},
   "source": [
    "### Identifying missing data patterns\n",
    "\n",
    "We compute the percentage of missing values for every column to understand the overall data quality and detect variables that may not be useful for the model.  \n",
    "This step is important because features with extremely high levels of missing data can distort the learning process and should be removed or handled carefully before defining the final dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10abb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_percent = df_work.isna().mean().sort_values(ascending=False) * 100\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "na_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13887b5",
   "metadata": {},
   "source": [
    "### Removing columns with no usable information\n",
    "\n",
    "We drop features that contain either 100% missing values or extremely high levels of missing data.  \n",
    "Such variables do not provide any meaningful signal for predicting availability and only add noise, making the dataset unnecessarily large and harder to process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed61ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = []\n",
    "\n",
    "cols_to_drop += [\"adjusted_price\", \"price\"]\n",
    "\n",
    "cols_to_drop += [\n",
    "    \"host_about\",\n",
    "    \"host_neighbourhood\",\n",
    "    \"neighborhood_overview\"\n",
    "]\n",
    "\n",
    "cols_to_drop = [c for c in cols_to_drop if c in df_work.columns]\n",
    "\n",
    "df_work = df_work.drop(columns=cols_to_drop)\n",
    "df_work.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1bded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work = df_work.dropna(subset=[\"available\"])\n",
    "\n",
    "df_work = df_work.dropna(subset=[\"listing_id\", \"date\"])\n",
    "\n",
    "df_work.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce282b8d",
   "metadata": {},
   "source": [
    "### Removing identification fields and non-informative text\n",
    "\n",
    "We remove ID columns and descriptive text fields because they do not provide meaningful predictive information for availability.  \n",
    "These features are arbitrary or subjective, and keeping them would add noise and increase the risk of overfitting without improving the model’s ability to learn relevant patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b655a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = [c for c in df_work.columns if c.endswith(\"_id\") or c == \"id\"]\n",
    "\n",
    "manual_drop = [\n",
    "    \"source\",       \n",
    "    \"name\",         \n",
    "    \"description\",  \n",
    "    \"host_name\",   \n",
    "]\n",
    "\n",
    "manual_drop = [c for c in manual_drop if c in df_work.columns]\n",
    "\n",
    "cols_to_drop = sorted(set(id_cols + manual_drop))\n",
    "\n",
    "df_work = df_work.drop(columns=cols_to_drop)\n",
    "\n",
    "df_work.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceac55a",
   "metadata": {},
   "source": [
    "### Defining target and feature matrices\n",
    "\n",
    "We separate the target variable `available` from the rest of the dataset to create `y` (labels) and `X` (predictors).  \n",
    "This step is essential fo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943b0bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_work[\"available\"]\n",
    "\n",
    "X = df_work.drop(columns=[\"available\"])\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a8b5c",
   "metadata": {},
   "source": [
    "### Diagnosing missing values in the feature set\n",
    "\n",
    "We calculate both the absolute and percentage of missing values for each feature in `X`.  \n",
    "This overview is important because it highlights which variables may require special handling during preprocessing and helps ensure that the model is trained on reliable and informative data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab6cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "missing_counts = X.isna().sum().sort_values(ascending=False)\n",
    "\n",
    "missing_percent = (X.isna().mean() * 100).sort_values(ascending=False)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    \"missing_count\": missing_counts,\n",
    "    \"missing_percent\": missing_percent\n",
    "})\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "missing_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a74bf1",
   "metadata": {},
   "source": [
    "### Inspecting numerical feature ranges\n",
    "\n",
    "We generate descriptive statistics for all numerical features to understand their minimum, maximum, and distributional properties.  \n",
    "This step is important because unusually large or small values can indicate potential outliers, which may distort the model’s learning process if not addressed properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe4edfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "summary_stats = X[num_cols].describe().transpose()\n",
    "\n",
    "summary_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb805c7",
   "metadata": {},
   "source": [
    "### Identifying potentially problematic numerical features\n",
    "\n",
    "We isolate numerical columns that may contain extreme or inconsistent values based on domain knowledge and initial statistics.  \n",
    "Examining these features is important because outliers in variables such as nights, latitude/longitude, or host listing counts can bias the model and reduce its ability to learn meaningful availability patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fdef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "suspect_cols = [\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"maximum_nights\",\n",
    "    \"minimum_minimum_nights\",\n",
    "    \"maximum_minimum_nights\",\n",
    "    \"minimum_maximum_nights\",\n",
    "    \"maximum_maximum_nights\",\n",
    "    \"minimum_nights_avg_ntm\",\n",
    "    \"maximum_nights_avg_ntm\",\n",
    "    \"host_listings_count\",\n",
    "    \"host_total_listings_count\",\n",
    "    \"calculated_host_listings_count\",\n",
    "    \"calculated_host_listings_count_entire_homes\",\n",
    "    \"calculated_host_listings_count_private_rooms\",\n",
    "    \"calculated_host_listings_count_shared_rooms\",\n",
    "    \"beds\",\n",
    "    \"bedrooms\",\n",
    "    \"bathrooms\",\n",
    "]\n",
    "\n",
    "suspect_cols = [c for c in suspect_cols if c in df_work.columns]\n",
    "\n",
    "stats_suspects = df_work[suspect_cols].describe().T\n",
    "stats_suspects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d4c44",
   "metadata": {},
   "source": [
    "### Visualizing potential outliers before cleaning\n",
    "\n",
    "We generate boxplots for all suspect numerical columns to visualize extreme values and confirm which features contain significant outliers.\n",
    "\n",
    "This step is important because it provides a clear picture of irregular distributions before removing outliers in the following step, ensuring that the cleaning process is targeted and justified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_plots = len(suspect_cols)\n",
    "plt.figure(figsize=(12, 3 * num_plots))\n",
    "\n",
    "for i, col in enumerate(suspect_cols, 1):\n",
    "    plt.subplot(num_plots, 1, i)\n",
    "    plt.boxplot(df_work[col].dropna(), vert=False)\n",
    "    plt.title(f\"Before outlier removal: {col}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f689c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_no_outliers = df_work.copy()\n",
    "\n",
    "for col in suspect_cols:\n",
    "    series = df_no_outliers[col].dropna()\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "\n",
    "    \n",
    "    mask = (df_no_outliers[col].isna()) | ((df_no_outliers[col] >= lower) & (df_no_outliers[col] <= upper))\n",
    "    df_no_outliers = df_no_outliers[mask]\n",
    "\n",
    "\n",
    "df_work = df_no_outliers.reset_index(drop=True)\n",
    "\n",
    "df_work.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d7ef10",
   "metadata": {},
   "source": [
    "### Visualizing distributions after outlier removal\n",
    "\n",
    "We plot the same numerical features again to verify how their distributions changed after removing outliers.  \n",
    "This comparison is important because it confirms that extreme or inconsistent values were effectively filtered out without distorting the underlying structure of the data that the model needs to learn from\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d6283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots = len(suspect_cols)\n",
    "plt.figure(figsize=(12, 3 * num_plots))\n",
    "\n",
    "for i, col in enumerate(suspect_cols, 1):\n",
    "    plt.subplot(num_plots, 1, i)\n",
    "    plt.boxplot(df_work[col].dropna(), vert=False)\n",
    "    plt.title(f\"After outlier removal: {col}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d4bad5",
   "metadata": {},
   "source": [
    "## Preprocessing strategy and rationale\n",
    "\n",
    "We designed the preprocessing pipeline to transform the cleaned dataset into a format that a machine learning model can use to predict whether a listing is available on a given date. The main ideas are:\n",
    "\n",
    "#### 1. Numerical features\n",
    "- **What we did:** applied a `SimpleImputer(strategy=\"median\")` followed by `StandardScaler()` to all numerical variables (nights, availability windows, review counts, host listing counts, etc.).\n",
    "- **Why:**  \n",
    "  - The median is robust to remaining outliers and provides a sensible default when values are missing.  \n",
    "  - Scaling puts all numerical variables on a comparable range, which helps many models (especially linear and distance-based ones) learn more stable coefficients and reduces the impact of variables that naturally have larger scales (e.g. number_of_reviews vs. review_scores).\n",
    "\n",
    "#### 2. Categorical features\n",
    "- **What we did:** used `SimpleImputer(strategy=\"most_frequent\")` followed by `OneHotEncoder(handle_unknown=\"ignore\")` for all object/bool columns (e.g. neighbourhood, room_type, amenities, instant_bookable).\n",
    "- **Why:**  \n",
    "  - Filling missing categories with the most frequent label preserves the distribution and avoids creating artificial categories.  \n",
    "  - One-hot encoding converts each category into a set of binary indicators, allowing the model to capture differences across property types, neighbourhood groups, and host characteristics.  \n",
    "  - `handle_unknown=\"ignore\"` ensures the pipeline can handle new/unseen categories at prediction time without failing.\n",
    "\n",
    "#### 3. Date features\n",
    "- **What we did:** created a custom transformer that:\n",
    "  - From `date` extracts `day`, `month` and `dayofweek`.  \n",
    "  - From `host_since`, `first_review` and `last_review` computes the number of days since each event (e.g. `hosting_days`, `days_since_first_review`, `days_since_last_review`).  \n",
    "  - Drops the original raw date columns and then applies median imputation and scaling to the derived numerical features.\n",
    "- **Why:**  \n",
    "  - Availability is time-dependent: seasonality (month, weekday) and host/listing “maturity” (how long it has been active) are likely to influence whether a listing tends to be booked or free.  \n",
    "  - Converting dates to numeric durations allows models to capture these temporal patterns directly, instead of treating dates as unstructured strings\n",
    "\n",
    "#### 4. Integrated preprocessing\n",
    "- **What we did:** combined the numeric, categorical, and date pipelines into a single `ColumnTransformer` called `preprocessor`.\n",
    "- **Why:**  \n",
    "  - Having a single preprocessing object ensures that all transformations are applied consistently during training and inference.  \n",
    "  - It cleanly separates the roles of each feature type and makes the pipeline easy to plug into any downstream model (e.g. logistic regression, tree-based models) without changing the preprocesing code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f798155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# 1. CUSTOM DATE TRANSFORMER\n",
    "\n",
    "class DateFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, date_cols):\n",
    "        self.date_cols = date_cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        today = pd.Timestamp(\"today\")\n",
    "\n",
    "        # Main date (the prediction date)\n",
    "        if \"date\" in self.date_cols:\n",
    "            X[\"date\"] = pd.to_datetime(X[\"date\"], errors=\"coerce\")\n",
    "            X[\"day\"] = X[\"date\"].dt.day\n",
    "            X[\"month\"] = X[\"date\"].dt.month\n",
    "            X[\"dayofweek\"] = X[\"date\"].dt.dayofweek\n",
    "\n",
    "        # Host since\n",
    "        if \"host_since\" in self.date_cols:\n",
    "            X[\"host_since\"] = pd.to_datetime(X[\"host_since\"], errors=\"coerce\")\n",
    "            X[\"hosting_days\"] = (today - X[\"host_since\"]).dt.days\n",
    "\n",
    "        # First review\n",
    "        if \"first_review\" in self.date_cols:\n",
    "            X[\"first_review\"] = pd.to_datetime(X[\"first_review\"], errors=\"coerce\")\n",
    "            X[\"days_since_first_review\"] = (today - X[\"first_review\"]).dt.days\n",
    "\n",
    "        # Last review\n",
    "        if \"last_review\" in self.date_cols:\n",
    "            X[\"last_review\"] = pd.to_datetime(X[\"last_review\"], errors=\"coerce\")\n",
    "            X[\"days_since_last_review\"] = (today - X[\"last_review\"]).dt.days\n",
    "\n",
    "        # Drop original date columns\n",
    "        return X.drop(columns=self.date_cols)\n",
    "\n",
    "# 2. DETECT COLUMNS\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n",
    "\n",
    "date_cols = [\"date\", \"host_since\", \"first_review\", \"last_review\"]\n",
    "date_cols = [c for c in date_cols if c in X.columns]\n",
    "\n",
    "# 3. DEFINE TRANSFORMERS\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "date_pipeline = Pipeline([\n",
    "    (\"date_transform\", DateFeatures(date_cols=date_cols)),\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# 4. COMBINE INTO COLUMN TRANSFORMER\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipeline, numeric_cols),\n",
    "        (\"cat\", categorical_pipeline, categorical_cols),\n",
    "        (\"date\", date_pipeline, date_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "preprocessor\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
